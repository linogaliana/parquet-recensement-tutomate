---
title: "Exemples d'utilisation des données du recensement au format `Parquet`"
author:
    - Lino Galiana
    - Mélina Hillion
format: 
   html:
     df-print: paged
---

Ce tutoriel vise à offrir une approche complémentaire
au guide d'utilisation des données du recensement au format `Parquet`
publié sur [https://ssphub.netlify.app](https://ssphub.netlify.app/post/parquetrp/)
pour accompagner la diffusion de celles-ci par l'Insee. 

Il s'agit d'un tutoriel préparé pour l'atelier [`tuto@mate`](https://mate-shs.cnrs.fr/actions/tutomate/tuto62_parquet_galiana/) à l'EHESS le 5 novembre 2024. Ce tutoriel est exclusivement en `R`. Les slides de la présentation sont disponibles ci-dessous:

<details>
<summary>

Dérouler les _slides_ ci-dessous ou [cliquer ici](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/complete.html#/partie-1-contr%C3%B4le-de-version-avec-git) pour afficher les slides en plein écran.

</summary>

```{=html}
<div class="sourceCode" id="cb1"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><iframe class="sourceCode yaml code-with-copy" src="https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/complete.html#/partie-1-contr%C3%B4le-de-version-avec-git"></iframe></div>
```


</details>

Il propose des exemples variés pour illustrer la simplicité d'usage du format `Parquet`. Parmi ceux-ci:

- Recenser les Niçois qui ont emménagé depuis 2021 (@lst-nice-emmenagement)


# Librairies utilisées

Ce tutoriel utilisera plusieurs librairies {{< fa brands r-project >}}. Celles-ci peuvent être importées ainsi[^renv]

[^renv]: Si vous avez clôné le dépôt disponible sur [`Github`](https://github.com/linogaliana/parquet-recensement-tutomate) {{< fa brands github >}}, un environnement virtuel `renv` vous permet de recréer la même configuration logicielle que celle utilisée pour générer cette page. Pour cela, il suffit de faire `renv::restore()`. 


```{r}
#| output: false
library(duckdb)
library(dplyr)
library(stringr)
library(glue)
library(dplyr)
library(cartiflette)
library(ggplot2)
library(sf)
library(gt)
```


# Téléchargement des fichiers

Pour commencer, nous allons télécharger les fichiers depuis internet pour limiter les échanges réseaux. Comme nous le verrons ultérieurement, ce n'est en fait pas indispensable car `duckdb` optimise les données téléchargées à chaque requête. 


```{r}
#| output: false
#| code-fold: true
#| code-summary: "Voir le code pour télécharger les données"

options(timeout = max(300, getOption("timeout"))) #<1>

download_if_not_exists <- function(url, filename) {
  if (!file.exists(filename)) {
    download.file(url, filename)
    message(paste("Downloaded:", filename))
  } else {
    message(paste("File already exists:", filename))
  }
}


url_table_logement <- "https://static.data.gouv.fr/resources/recensement-de-la-population-fichiers-detail-logements-ordinaires-en-2020-1/20231023-123618/fd-logemt-2020.parquet"
url_table_individu <- "https://static.data.gouv.fr/resources/recensement-de-la-population-fichiers-detail-individus-localises-au-canton-ou-ville-2020-1/20231023-122841/fd-indcvi-2020.parquet"
url_doc_logement <- "https://www.data.gouv.fr/fr/datasets/r/c274705f-98db-4d9b-9674-578e04f03198"
url_doc_individu <- "https://www.data.gouv.fr/fr/datasets/r/1c6c6ab2-b766-41a4-90f0-043173d5e9d1"


filename_table_logement <- str_split_i(url_table_logement, "/", -1)
filename_table_individu <- str_split_i(url_table_individu, "/", -1)

# Télécharge les fichiers
download_if_not_exists(url_table_logement, filename_table_logement)
download_if_not_exists(url_table_individu, filename_table_individu)
download_if_not_exists(url_doc_logement, "dictionnaire_variables_logemt_2020.csv")
download_if_not_exists(url_doc_individu, "dictionnaire_variables_indcvi_2020.csv")
```
1. Ceci est nécessaire pour éviter une erreur si le téléchargement est un peu lent.

Nous aurons également besoin pour quelques illustrations d'un fond de carte des départements. 
Celui-ci peut être simplement récupéré grâce au _package_ `cartiflette`[^cartiflette]

[^cartiflette]: Pour en savoir plus sur ce projet, se rendre sur la [documentation](https://inseefrlab.github.io/cartiflette-website/) du projet.

```{r}
#| output: false
#| code-fold: true
#| code-summary: "Voir le code pour récupérer ce fond de carte"
departements <- carti_download(
  values="France",
  crs=4326,
  borders="DEPARTEMENT",
  vectorfile_format="geojson",
  filter_by="FRANCE_ENTIERE_DROM_RAPPROCHES",
  source="EXPRESS-COG-CARTO-TERRITOIRE",
  year=2022,
)
```

```{r}
#| code-fold: true
#| code-summary: "Voir le code pour faire cette carte"
ggplot(departements) +
  geom_sf() +
  theme_void()
```


# Création de la base de données

En principe, `duckdb` fonctionne à la manière d'une base de données. Autrement dit, on définit une base de données et effectue des requêtes (SQL ou verbes `tidyverse`) dessus. Pour créer une base de données, il suffit de faire un `read_parquet` avec le chemin du fichier.  

La base de données se crée tout simplement de la manière suivante:

```{r}
con <- dbConnect(duckdb()) 
```

Celle-ci peut être utilisée de plusieurs manières. En premier lieu, par le biais d'une requête SQL. `dbGetQuery` permet d'avoir le résultat sous forme de _dataframe_ puisque la requête est déléguée à l'utilitaire `duckdb` qui est embarqué dans les fichiers de la librairie


```{r}
#| output: false
out <- dbGetQuery(
  con,
  glue(  
    'SELECT * FROM read_parquet("{filename_table_individu}") LIMIT 5'
  )
)
out
```


La chaîne d'exécution ressemble ainsi à celle-ci:

![](/img/duckdb-delegation1.png){fig-align="center"}

Même si `DuckDB` simplifie l'utilisation du SQL en proposant de nombreux verbes auxquels on est familier en `R` ou `Python`, SQL n'est néanmoins pas toujours le langage le plus pratique pour chaîner des opérations nombreuses. Pour ce type de besoin,  le `tidyverse` offre une grammaire riche et cohérente. Il est tout à fait possible d'interfacer une base `duckdb` au `tidyverse`. On pourra donc utiliser nos verbes préférés (`mutate`, `filter`, etc.) sur un objet `duckdb`: une phase préliminaire de traduction en SQL sera automatiquement mise en oeuvre:

![](/img/duckdb-delegation2.png){fig-align="center"}


```{r}
#| message: false
table_logement <- tbl(con, glue('read_parquet("{filename_table_logement}")'))
table_individu <- tbl(con, glue('read_parquet("{filename_table_individu}")'))
```

L'équivalent `tidyverse` de la requête précédente est la fonction `head`

```{r}
table_individu %>% head(5)
```

Le fait de passer par l'intermédiaire de `duckdb` et un fichier `Parquet` permet d'optimiser les besoins mémoire de `R`. En effet, il n'est pas nécessaire d'ouvrir un fichier dans son ensemble, le transformer en objet `R` pour n'utiliser qu'une partie des données. Nous verrons ultérieurement la manière dont les besoins mémoires sont minimisés grâce au combo `duckdb` & `Parquet`.

Enfin, nous pouvons importer les dictionnaires des variables qui pourront nous servir ultérieurement:

```{r}
#| output: false
documentation_logement <- readr::read_csv2("dictionnaire_variables_logemt_2020.csv")
documentation_individus <- readr::read_csv2("dictionnaire_variables_indcvi_2020.csv")
```


# Ouvrir un fichier `Parquet`

## Requêtes sur les colonnes (`SELECT`)

L'une des forces du format `Parquet` est de simplifier l'import de fichiers volumineux qui ne comportent que quelques colonnes nous intéressant. Par exemple, la table des individus comporte 88 colonnes, il est peu probable qu'une seule analyse s'intéresse à toutes celles-ci (ou elle risque d'être fort indigeste).

![](/img/parquet-table2-enriched.png)

Comme cela est illustré dans @tip-optimisation-duckdb, la différence de volumétrie entre un fichier non filtré et un fichier filtré est importante. 

::: {.panel-tabset}

## `DuckDB` via `tidyverse`

```{r}
table_individu %>%
  select(poids = IPONDI, AGED, VOIT) %>%
  head(10)
```


## `DuckDB` exclusivement

```{r}
query <- glue(
  "FROM read_parquet(\"{filename_table_individu}\") ",
  "SELECT IPONDI AS poids, AGED, VOIT ",
  "LIMIT 10"
)
dbGetQuery(
  con,
  query
)
```

:::

::: {#tip-optimisation-duckdb .callout-tip collapse="true"}
## Comprendre l'optimisation permise par `Parquet` et `DuckDB`

Pour réduire la volumétrie des données importées, il est possible de mettre en oeuvre deux stratégies:

- N'importer qu'un nombre limité de colonnes
- N'importer qu'un nombre limité de lignes

Comme cela a été évoqué dans les _slides_, le format `Parquet` est particulièrement optimisé pour le premier besoin. C'est donc généralement la première optimisation mise en oeuvre. Pour s'en convaincre on peut regarder la taille des données importées dans deux cas:

- On utilise beaucoup de lignes mais peu de colonnes
- On utilise beaucoup de colonnes mais peu de lignes

Pour cela, nous utilisons la fonction SQL `EXPLAIN ANALYZE` disponible dans `duckdb`. Elle décompose le plan d'exécution de `duckdb`, ce qui nous permettra de comprendre la stratégie d'optimisation. Elle permet aussi de connaître le volume de données importées lorsqu'on récupère un fichier d'internet. En effet, `duckdb` est malin: plutôt que de télécharger un fichier entier pour n'en lire qu'une partie, la librairie est capable de n'importer que les blocs du fichier qui l'intéresse. 

Ceci nécessite l'utilisation de l'extension `httpfs` (un peu l'équivalent des `library` de `R` en `duckdb`). Elle s'installe et s'utilise de la manière suivante

```{r}
#| output: false
dbExecute(
  con,
  glue(
    "INSTALL httpfs;",
    "LOAD httpfs;"
  )
)
```

Demandons à `DuckDB` d'exécuter la requête _"beaucoup de colonnes, pas beaucoup de lignes"_
et regardons le plan d'exécution et les informations données par `DuckDB`:

<details>

<summary>
Voir le plan : _"beaucoup de colonnes, pas beaucoup de lignes"_
</summary>

```{r}
plan <- dbGetQuery(
  con,
  glue(  
    'EXPLAIN ANALYZE ',
    'SELECT * FROM read_parquet("{url_table_logement}") LIMIT 5'
  )
)
```

```{r}
print(plan)
```

</details>

<details>

<summary>
Voir le plan : _"pas de colonnes, beaucoup de lignes"_
</summary>

```{r}
plan <- dbGetQuery(
  con,
  glue(  
    'EXPLAIN ANALYZE ',
    'SELECT IPONDI AS poids, AGED, VOIT FROM read_parquet("{url_table_individu}") LIMIT 10000'
  )
)
```

```{r}
print(plan)
```

</details>

La comparaison de ces plans d'exécution montre l'intérêt de faire un filtre sur les colonnes : les besoins computationnels sont drastiquement diminués. Le filtre sur les lignes n'arrive que dans un second temps, une fois les colonnes sélectionnées. 

Pourquoi seulement un rapport de 1 à 4 entre le poids des deux fichiers ? C'est parce que nos requêtes comportent toute deux la variable `IPONDI` (les poids à utiliser pour extrapoler l'échantillon à la population) qui est à haute précision là où beaucoup d'autres colonnes comportent un nombre réduit de modalités et sont donc peu volumineuses.

:::

DuckDB propose également des fonctionnalités pour extraire des colonnes à travers des [expressions régulières](https://fr.wikipedia.org/wiki/Expression_r%C3%A9guli%C3%A8re). Cette approche est également possible avec le `tidyverse`

::: {.panel-tabset}

## `DuckDB` via `tidyverse`

```{r}
table_individu %>%
  select(poids = IPONDI, contains("AGE")) %>%
  head(10)
```

## `DuckDB` exclusivement

```{r}
dbGetQuery(
  con,
  glue(
    "FROM read_parquet(\"{filename_table_individu}\") ",
    "SELECT IPONDI AS poids, COLUMNS('.*AGE.*') ",
    "LIMIT 10"
  )
)
```

:::

## Requêtes sur les lignes (`WHERE`)

::: {.panel-tabset}

## `DuckDB` via `tidyverse`

```{r}
table_individu %>%
  filter(DEPT %in% c("11", "31", "34")) %>%
  head(10)
```

## `DuckDB` exclusivement

```{r}
dbGetQuery(
  con,
  glue(
    "FROM read_parquet(\"{filename_table_individu}\") ",
    "SELECT IPONDI, AGED, DEPT ",
    "WHERE DEPT IN ('11', '31', '34') ",
    "LIMIT 10")
)
```

:::


Les filtres sur les observations peuvent être faits à partir de critères sur plusieurs colonnes. Par exemple, pour ne conserver que les observations de la ville de Nice où la date d’emménagement est postérieure à 2020, la requête suivante peut être utilisée :

::: {.panel-tabset}

## `DuckDB` via `tidyverse`

```{r}
#| lst-label: lst-nice-emmenagement
#| lst-cap: "Ne conserver que les Niçois qui ont emménagé depuis 2021"
table_logement %>% filter(COMMUNE == "06088", AEMM > 2020)
```

## `DuckDB` exclusivement

```{r}
dbGetQuery(
  con,
  glue(
    "FROM read_parquet(\"{filename_table_logement}\") ",
    "SELECT * ",
    "WHERE COMMUNE = '06088' and AEMM > 2020 ",
    "LIMIT 10"
  )
)
```

:::



