---
title: "Exemples d'utilisation des données du recensement au format `Parquet`"
author:
    - Lino Galiana
format: 
   html:
     df-print: paged
---

Ce tutoriel vise à offrir une approche complémentaire
au guide d'utilisation des données du recensement au format `Parquet`
publié sur [https://ssphub.netlify.app](https://ssphub.netlify.app/post/parquetrp/)
pour accompagner la diffusion de celles-ci par l'Insee. 

Il s'agit d'un tutoriel préparé pour l'atelier [`tuto@mate`](https://mate-shs.cnrs.fr/actions/tutomate/tuto62_parquet_galiana/) à l'EHESS le 5 novembre 2024. Ce tutoriel est exclusivement en `R`. Les slides de la présentation sont disponibles ci-dessous:

<details>
<summary>

Dérouler les _slides_ ci-dessous ou [cliquer ici](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/complete.html#/partie-1-contr%C3%B4le-de-version-avec-git) pour afficher les slides en plein écran.

</summary>

```{=html}
<div class="sourceCode" id="cb1"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><iframe class="sourceCode yaml code-with-copy" src="https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/complete.html#/partie-1-contr%C3%B4le-de-version-avec-git"></iframe></div>
```


</details>

Il propose des exemples variés pour illustrer la simplicité d'usage du format `Parquet`. Parmi ceux-ci:

- Recenser les Niçois qui ont emménagé depuis 2021 (@lst-nice-emmenagement)


# Librairies utilisées

Ce tutoriel utilisera plusieurs librairies {{< fa brands r-project >}}. Celles-ci peuvent être importées ainsi[^renv]

[^renv]: Si vous avez clôné le dépôt disponible sur [`Github`](https://github.com/linogaliana/parquet-recensement-tutomate) {{< fa brands github >}}, un environnement virtuel `renv` vous permet de recréer la même configuration logicielle que celle utilisée pour générer cette page. Pour cela, il suffit de faire `renv::restore()`. 


```{r}
#| output: false
library(duckdb)
library(dplyr)
library(stringr)
library(glue)
library(dplyr)
library(cartiflette)
library(ggplot2)
library(sf)
library(gt)
```


# Téléchargement des fichiers

Pour commencer, nous allons télécharger les fichiers depuis internet pour limiter les échanges réseaux. Comme nous le verrons ultérieurement, ce n'est en fait pas indispensable car `duckdb` optimise les données téléchargées à chaque requête. 


```{r}
#| output: false
#| code-fold: true
#| code-summary: "Voir le code pour télécharger les données"

options(timeout = max(300, getOption("timeout"))) #<1>

download_if_not_exists <- function(url, filename) {
  if (!file.exists(filename)) {
    download.file(url, filename)
    message(paste("Downloaded:", filename))
  } else {
    message(paste("File already exists:", filename))
  }
}


url_table_logement <- "https://static.data.gouv.fr/resources/recensement-de-la-population-fichiers-detail-logements-ordinaires-en-2020-1/20231023-123618/fd-logemt-2020.parquet"
url_table_individu <- "https://static.data.gouv.fr/resources/recensement-de-la-population-fichiers-detail-individus-localises-au-canton-ou-ville-2020-1/20231023-122841/fd-indcvi-2020.parquet"
url_doc_logement <- "https://www.data.gouv.fr/fr/datasets/r/c274705f-98db-4d9b-9674-578e04f03198"
url_doc_individu <- "https://www.data.gouv.fr/fr/datasets/r/1c6c6ab2-b766-41a4-90f0-043173d5e9d1"


filename_table_logement <- str_split_i(url_table_logement, "/", -1)
filename_table_individu <- str_split_i(url_table_individu, "/", -1)

# Télécharge les fichiers
download_if_not_exists(url_table_logement, filename_table_logement)
download_if_not_exists(url_table_individu, filename_table_individu)
download_if_not_exists(url_doc_logement, "dictionnaire_variables_logemt_2020.csv")
download_if_not_exists(url_doc_individu, "dictionnaire_variables_indcvi_2020.csv")
```
1. Ceci est nécessaire pour éviter une erreur si le téléchargement est un peu lent.

Nous aurons également besoin pour quelques illustrations d'un fond de carte des départements. 
Celui-ci peut être simplement récupéré grâce au _package_ `cartiflette`[^cartiflette]

[^cartiflette]: Pour en savoir plus sur ce projet, se rendre sur la [documentation](https://inseefrlab.github.io/cartiflette-website/) du projet.

```{r}
#| output: false
#| code-fold: true
#| code-summary: "Voir le code pour récupérer ce fond de carte"
departements <- carti_download(
  values="France",
  crs=4326,
  borders="DEPARTEMENT",
  vectorfile_format="geojson",
  filter_by="FRANCE_ENTIERE_DROM_RAPPROCHES",
  source="EXPRESS-COG-CARTO-TERRITOIRE",
  year=2022,
)
```

```{r}
#| code-fold: true
#| code-summary: "Voir le code pour faire cette carte"
ggplot(departements) +
  geom_sf() +
  theme_void()
```


# Création de la base de données

En principe, `duckdb` fonctionne à la manière d'une base de données. Autrement dit, on définit une base de données et effectue des requêtes (SQL ou verbes `tidyverse`) dessus. Pour créer une base de données, il suffit de faire un `read_parquet` avec le chemin du fichier.  

La base de données se crée tout simplement de la manière suivante:

```{r}
con <- dbConnect(duckdb()) 
```

Celle-ci peut être utilisée de plusieurs manières. En premier lieu, par le biais d'une requête SQL. `dbGetQuery` permet d'avoir le résultat sous forme de _dataframe_ puisque la requête est déléguée à l'utilitaire `duckdb` qui est embarqué dans les fichiers de la librairie


```{r}
#| output: false
out <- dbGetQuery(
  con,
  glue(  
    'SELECT * FROM read_parquet("{filename_table_individu}") LIMIT 5'
  )
)
out
```


La chaîne d'exécution ressemble ainsi à celle-ci:

![](/img/duckdb-delegation1.png){fig-align="center"}

Même si `DuckDB` simplifie l'utilisation du SQL en proposant de nombreux verbes auxquels on est familier en `R` ou `Python`, SQL n'est néanmoins pas toujours le langage le plus pratique pour chaîner des opérations nombreuses. Pour ce type de besoin,  le `tidyverse` offre une grammaire riche et cohérente. Il est tout à fait possible d'interfacer une base `duckdb` au `tidyverse`. On pourra donc utiliser nos verbes préférés (`mutate`, `filter`, etc.) sur un objet `duckdb`: une phase préliminaire de traduction en SQL sera automatiquement mise en oeuvre:

![](/img/duckdb-delegation2.png){fig-align="center"}


```{r}
#| message: false
table_logement <- tbl(con, glue('read_parquet("{filename_table_logement}")'))
table_individu <- tbl(con, glue('read_parquet("{filename_table_individu}")'))
```

L'équivalent `tidyverse` de la requête précédente est la fonction `head`

```{r}
table_individu %>% head(5)
```

Le fait de passer par l'intermédiaire de `duckdb` et un fichier `Parquet` permet d'optimiser les besoins mémoire de `R`. En effet, il n'est pas nécessaire d'ouvrir un fichier dans son ensemble, le transformer en objet `R` pour n'utiliser qu'une partie des données. Nous verrons ultérieurement la manière dont les besoins mémoires sont minimisés grâce au combo `duckdb` & `Parquet`.

Enfin, nous pouvons importer les dictionnaires des variables qui pourront nous servir ultérieurement:

```{r}
#| output: false
documentation_logement <- readr::read_csv2("dictionnaire_variables_logemt_2020.csv")
documentation_individus <- readr::read_csv2("dictionnaire_variables_indcvi_2020.csv")
```


# Ouvrir un fichier `Parquet`

## Requêtes sur les colonnes (`SELECT`)

L'une des forces du format `Parquet` est de simplifier l'import de fichiers volumineux qui ne comportent que quelques colonnes nous intéressant. Par exemple, la table des individus comporte 88 colonnes, il est peu probable qu'une seule analyse s'intéresse à toutes celles-ci (ou elle risque d'être fort indigeste).

![](/img/parquet-table2-enriched.png)

Comme cela est illustré dans @tip-optimisation-duckdb, la différence de volumétrie entre un fichier non filtré et un fichier filtré est importante. 

::: {.panel-tabset}

## `DuckDB` via `tidyverse`

```{r}
table_individu %>%
  select(poids = IPONDI, AGED, VOIT) %>%
  head(10)
```


## `DuckDB` exclusivement

```{r}
query <- glue(
  "FROM read_parquet(\"{filename_table_individu}\") ",
  "SELECT IPONDI AS poids, AGED, VOIT ",
  "LIMIT 10"
)
dbGetQuery(
  con,
  query
)
```

:::

::: {#tip-optimisation-duckdb .callout-tip collapse="true"}
## Comprendre l'optimisation permise par `Parquet` et `DuckDB`

Pour réduire la volumétrie des données importées, il est possible de mettre en oeuvre deux stratégies:

- N'importer qu'un nombre limité de colonnes
- N'importer qu'un nombre limité de lignes

Comme cela a été évoqué dans les _slides_, le format `Parquet` est particulièrement optimisé pour le premier besoin. C'est donc généralement la première optimisation mise en oeuvre. Pour s'en convaincre on peut regarder la taille des données importées dans deux cas:

- On utilise beaucoup de lignes mais peu de colonnes
- On utilise beaucoup de colonnes mais peu de lignes

Pour cela, nous utilisons la fonction SQL `EXPLAIN ANALYZE` disponible dans `duckdb`. Elle décompose le plan d'exécution de `duckdb`, ce qui nous permettra de comprendre la stratégie d'optimisation. Elle permet aussi de connaître le volume de données importées lorsqu'on récupère un fichier d'internet. En effet, `duckdb` est malin: plutôt que de télécharger un fichier entier pour n'en lire qu'une partie, la librairie est capable de n'importer que les blocs du fichier qui l'intéresse. 

Ceci nécessite l'utilisation de l'extension `httpfs` (un peu l'équivalent des `library` de `R` en `duckdb`). Elle s'installe et s'utilise de la manière suivante

```{r}
#| output: false
dbExecute(
  con,
  glue(
    "INSTALL httpfs;",
    "LOAD httpfs;"
  )
)
```

Demandons à `DuckDB` d'exécuter la requête _"beaucoup de colonnes, pas beaucoup de lignes"_
et regardons le plan d'exécution et les informations données par `DuckDB`:

<details>

<summary>
Voir le plan : _"beaucoup de colonnes, pas beaucoup de lignes"_
</summary>

```{r}
plan <- dbGetQuery(
  con,
  glue(  
    'EXPLAIN ANALYZE ',
    'SELECT * FROM read_parquet("{url_table_logement}") LIMIT 5'
  )
)
```

```{r}
print(plan)
```

</details>

<details>

<summary>
Voir le plan : _"pas de colonnes, beaucoup de lignes"_
</summary>

```{r}
plan <- dbGetQuery(
  con,
  glue(  
    'EXPLAIN ANALYZE ',
    'SELECT IPONDI AS poids, AGED, VOIT FROM read_parquet("{url_table_individu}") LIMIT 10000'
  )
)
```

```{r}
print(plan)
```

</details>

La comparaison de ces plans d'exécution montre l'intérêt de faire un filtre sur les colonnes : les besoins computationnels sont drastiquement diminués. Le filtre sur les lignes n'arrive que dans un second temps, une fois les colonnes sélectionnées. 

Pourquoi seulement un rapport de 1 à 4 entre le poids des deux fichiers ? C'est parce que nos requêtes comportent toute deux la variable `IPONDI` (les poids à utiliser pour extrapoler l'échantillon à la population) qui est à haute précision là où beaucoup d'autres colonnes comportent un nombre réduit de modalités et sont donc peu volumineuses.

:::

DuckDB propose également des fonctionnalités pour extraire des colonnes à travers des [expressions régulières](https://fr.wikipedia.org/wiki/Expression_r%C3%A9guli%C3%A8re). Cette approche est également possible avec le `tidyverse`

::: {.panel-tabset}

## `DuckDB` via `tidyverse`

```{r}
table_individu %>%
  select(poids = IPONDI, contains("AGE")) %>%
  head(10)
```

## `DuckDB` exclusivement

```{r}
dbGetQuery(
  con,
  glue(
    "FROM read_parquet(\"{filename_table_individu}\") ",
    "SELECT IPONDI AS poids, COLUMNS('.*AGE.*') ",
    "LIMIT 10"
  )
)
```

:::

## Requêtes sur les lignes (`WHERE`)

::: {.panel-tabset}

## `DuckDB` via `tidyverse`

```{r}
table_individu %>%
  filter(DEPT %in% c("11", "31", "34")) %>%
  head(10)
```

## `DuckDB` exclusivement

```{r}
dbGetQuery(
  con,
  glue(
    "FROM read_parquet(\"{filename_table_individu}\") ",
    "SELECT IPONDI, AGED, DEPT ",
    "WHERE DEPT IN ('11', '31', '34') ",
    "LIMIT 10")
)
```

:::


Les filtres sur les observations peuvent être faits à partir de critères sur plusieurs colonnes. Par exemple, pour ne conserver que les observations de la ville de Nice où la date d’emménagement est postérieure à 2020, la requête suivante peut être utilisée :

::: {.panel-tabset}

## `DuckDB` via `tidyverse`

```{r}
#| lst-label: lst-nice-emmenagement
#| lst-cap: "Ne conserver que les Niçois qui ont emménagé depuis 2021"
table_logement %>% filter(COMMUNE == "06088", AEMM > 2020)
```

## `DuckDB` exclusivement

```{r}
dbGetQuery(
  con,
  glue(
    "FROM read_parquet(\"{filename_table_logement}\") ",
    "SELECT * ",
    "WHERE COMMUNE = '06088' and AEMM > 2020 ",
    "LIMIT 10"
  )
)
```

:::


# Statistiques agrégées

## Exemples sans groupes

La fonction `DISTINCT` appliquée à la variable `ARM` permet d’extraire la liste des codes arrondissements présents dans la base de données.


::: {.panel-tabset}

## `DuckDB` via `tidyverse`

```{r}
table_logement %>%
  filter(str_detect(ARM, "ZZZZZ", negate = TRUE)) %>%
  summarise(ARM = distinct(ARM)) %>%
  arrange(ARM)
```

## `DuckDB` exclusivement

```{r}
query <- glue_sql(
    "FROM read_parquet({filename_table_logement}) ",
    "SELECT DISTINCT(ARM) ",
    "WHERE NOT CONTAINS(ARM, 'ZZZZZ') ",
    "ORDER BY ARM",
    .con=con
)
paste(dbGetQuery(con, query)$ARM, collapse = ", ")
```

:::

Il est possible d’extraire des statistiques beaucoup plus raffinées par le biais d’une requête SQL plus complexe. Par exemple pour calculer le nombre d’habitants de Toulouse qui ont changé de logement en un an:


::: {.panel-tabset}

## `DuckDB` via `tidyverse`

```{r}
#| lst-label: lst-toulouse-logements
#| lst-cap: "Nombre de Toulousains qui ont changé de logement en un an"
table_logement %>%
  filter(COMMUNE == '31555', !IRANM %in% c('1', 'Z'), INPER != "Y") %>%
  mutate(INPER = as.integer(INPER)) %>%
  summarise(habitants_toulouse_demenagement = as.integer(sum(IPONDL * INPER)))
```

## `DuckDB` exclusivement

```{r}
query <- paste(
  glue("FROM read_parquet(\"{filename_table_logement}\")"),
  "SELECT CAST(SUM(IPONDL*CAST(INPER AS INT)) AS INT) ",
  "AS habitants_toulouse_demenagement",
  "WHERE COMMUNE == '31555' AND IRANM NOT IN ('1', 'Z') AND INPER != 'Y'",
  sep = " ")
dbGetQuery(con, query)
```

:::

## Statistiques par groupe

`SQL` et `dplyr` permettent d'aller loin dans la finesse des statistiques descriptives mises en oeuvre. 
Cela sera illustré à l'aide de plusieurs exemples réflétant des statistiques pouvant être construites grâce à ces données détaillées. 


### Exemple 1: pyramide des âges dans l'Aude, l'Hérault et le Gard

Le premier exemple est un comptage sur trois départements. Il illustre la démarche suivante:

1. On se restreint aux observations d'intérêt (ici 3 départements)
2. On applique la fonction `summarise` pour calculer une statistique par groupe, en l'occurrence la somme des pondérations
3. On retravaille les données

Ensuite, une fois que nos données sont récupérées dans `R`, on peut faire la figure avec `ggplot`

```{r}
#| lst-label: lst-sud-pyramide
#| lst-cap: "Pyramide des âges dans l'Aude, l'Hérault et le Gard"

pyramide_ages <- table_individu %>%
  filter(DEPT %in% c('11', '31', '34')) %>%
  group_by(AGED, departement = DEPT) %>%
  summarise(individus = sum(IPONDI), .groups = "drop") %>%
  arrange(departement, AGED)


ggplot(pyramide_ages, aes(x = AGED, y = individus)) +
  geom_bar(aes(fill = departement), stat = "identity") +
  geom_vline(xintercept = 18, color = "grey", linetype = "dashed") +
  facet_wrap(~departement, scales = "free_y", nrow = 3) +
  theme_minimal() +
  labs(y = "Individus recensés", x = "Âge")
```


### Exemple 2: répartition des plus de 60 ans par département

L'objectif de ce deuxième exemple est d'illustrer la construction d'une statistique un peu plus complexe et la manière de projeter celle-ci sur une carte.

Pour avoir la répartition des plus de 60 ans par département, quelques lignes de `dplyr` suffisent:

```{r}
part_population_60_plus <- table_individu %>%
  group_by(DEPT) %>%
  summarise(
    total_population = sum(IPONDI), # Population totale
    population_60_plus = sum(IPONDI[AGED > 60]) # Population de plus de 60 ans
  ) %>%
  mutate(pourcentage_60_plus = population_60_plus / total_population * 100) %>%
  collect()

part_population_60_plus
```

Il ne reste plus qu'à projeter ceci sur une carte. Pour cela, un _join_ à notre fond de carte suffit. Comme les données sont agrégées et déjà dans `R`, il n'y a rien de spécifique à `duckdb` ici. 

```{r}
#| code-fold: true
#| code-summary: Association de part_population_60_plus au fond de carte des départements
# Joindre les données au fond de carte des départements
departements_60_plus_sf <- departements %>%
  inner_join(
    part_population_60_plus,
    by = c("INSEE_DEP" = "DEPT")
  )
```

Finalement, il ne reste plus qu'à produire la carte:

```{r}
ggplot(departements_60_plus_sf) +
    geom_sf(aes(fill = pourcentage_60_plus)) + 
    scale_fill_fermenter(n.breaks = 5, palette = "PuBuGn", direction = 1) + 
    theme_void() + 
    labs(
        title = "Part des personnes de plus de 60 ans par département",
        caption = "Source: Insee, Fichiers détails du recensement de la population",
        fill = "Part (%)"
    )
```


### Exemple 3: part des résidences secondaires et des logements vacants 

Il est tout à fait possible de faire des étapes antérieures de préparation de données, notamment de création de variables avec `mutate`.

L'exemple suivant illustre la préparation de données avant la construction de statistiques descriptives de la manière suivante:

1. Création d'une variable de département à partir du code commune
2. Décompte des logements par département

```{r}
parc_locatif <- table_logement %>%
  mutate(DEPT = substring(COMMUNE, 1, 3)) %>%
  mutate(
    DEPT = if_else(
      starts_with(DEPT, "97"),
      DEPT,
      substring(DEPT, 1, 2)
    )
  ) %>%
  group_by(DEPT, CATL) %>%
  summarise(n = sum(IPONDL)) %>%
  ungroup() %>%
  collect()
```

```{r}
# Jointure avec le fond de carte des départements
parc_locatif_sf <- departements %>%
  inner_join(
    parc_locatif,
    by = c("INSEE_DEP" = "DEPT"),
    relationship = "many-to-many" # on a des clés dupliquées dans le fond cartiflette (e.g. Ile de France) et dans le dataframe (4 valeurs par dep)
  ) %>%
  group_by(INSEE_DEP) %>%
  mutate(p = n/sum(n)) %>%
  ungroup
```


```{r}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Résidences secondaires"
#|   - "Logements vacants"

# Carte: Part des résidences secondaires
carte1 <- ggplot(parc_locatif_sf %>% filter(CATL == "3")) +
  geom_sf(aes(fill = p), color = "white") +
  scale_fill_fermenter(
    n.breaks = 5, 
    palette = "RdPu",
    direction = 1,
    labels = scales::label_percent(
      scale_cut = scales::cut_short_scale()
    )
  ) +
  theme_void() +
  labs(
    fill = "Part dans le\nparc de logement (%)",
    title = "Cartographie des résidences secondaires",
    caption = "Source: Insee, Fichiers détails du recensement de la population"
  )

# Carte: Part des logements vacants
carte2 <- ggplot(parc_locatif_sf %>% filter(CATL == "4")) +
  geom_sf(aes(fill = p), color = "white") +
  scale_fill_fermenter(
    n.breaks = 5, 
    palette = "RdPu",
    direction = 1,
    labels = scales::label_percent(
      scale_cut = scales::cut_short_scale()
    )
  ) +
  theme_void() +
  labs(
    fill = "Part dans le\nparc de logement (%)",
    title = "Cartographie des résidences secondaires",
    caption = "Source: Insee, Fichiers détails du recensement de la population"
  )


carte1
carte2
```



