[
  {
    "objectID": "tuto/r.html",
    "href": "tuto/r.html",
    "title": "Utilisation du format Parquet avec  illustré à partir de quelques exemples",
    "section": "",
    "text": "Ce tutoriel vise à offrir une approche complémentaire au guide d’utilisation des données du recensement au format Parquet publié sur https://ssphub.netlify.app pour accompagner la diffusion de celles-ci par l’Insee.\nIl s’agit d’un tutoriel préparé pour l’atelier tuto@mate à l’EHESS le 5 novembre 2024. Ce tutoriel est exclusivement en R. Les slides de la présentation sont disponibles ci-dessous:\nIl propose des exemples variés pour illustrer la simplicité d’usage du format Parquet. Parmi ceux-ci, à partir du recensement de la population:\nA partir de la base permanent des équipements (BPE):"
  },
  {
    "objectID": "tuto/r.html#requêtes-sur-les-colonnes-select",
    "href": "tuto/r.html#requêtes-sur-les-colonnes-select",
    "title": "Utilisation du format Parquet avec  illustré à partir de quelques exemples",
    "section": "Requêtes sur les colonnes (SELECT)",
    "text": "Requêtes sur les colonnes (SELECT)\nL’une des forces du format Parquet est de simplifier l’import de fichiers volumineux qui ne comportent que quelques colonnes nous intéressant. Par exemple, la table des individus comporte 88 colonnes, il est peu probable qu’une seule analyse s’intéresse à toutes celles-ci (ou elle risque d’être fort indigeste).\n\nComme cela est illustré dans Tip 1, la différence de volumétrie entre un fichier non filtré et un fichier filtré est importante.\n\n\nDuckDB via tidyverse\nDuckDB exclusivement\n\n\n\n\ntable_individu %&gt;%\n  select(poids = IPONDI, AGED, VOIT) %&gt;%\n  head(5) %&gt;%\n  collect()\n\n\n  \n\n\n\n\n\n\nglue(\n  \"SELECT IPONDI AS poids, AGED, VOIT \",\n  \"FROM read_parquet(\\\"{filename_table_individu}\\\") \"\n)\n\nSELECT IPONDI AS poids, AGED, VOIT FROM read_parquet(\"data/RPindividus.parquet\") \n\n\n\nquery &lt;- glue(\n  \"SELECT IPONDI AS poids, AGED, VOIT \",\n  \"FROM read_parquet(\\\"{filename_table_individu}\\\") \"\n)\ndbGetQuery(\n  con,\n  query %&gt;% head(10)\n)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nTip 1: Comprendre l’optimisation permise par Parquet et DuckDB\n\n\n\n\n\nPour réduire la volumétrie des données importées, il est possible de mettre en oeuvre deux stratégies:\n\nN’importer qu’un nombre limité de colonnes\nN’importer qu’un nombre limité de lignes\n\nComme cela a été évoqué dans les slides, le format Parquet est particulièrement optimisé pour le premier besoin. C’est donc généralement la première optimisation mise en oeuvre. Pour s’en convaincre on peut regarder la taille des données importées dans deux cas:\n\nOn utilise beaucoup de lignes mais peu de colonnes\nOn utilise beaucoup de colonnes mais peu de lignes\n\nPour cela, nous utilisons la fonction SQL EXPLAIN ANALYZE disponible dans duckdb. Elle décompose le plan d’exécution de duckdb, ce qui nous permettra de comprendre la stratégie d’optimisation. Elle permet aussi de connaître le volume de données importées lorsqu’on récupère un fichier d’internet. En effet, duckdb est malin: plutôt que de télécharger un fichier entier pour n’en lire qu’une partie, la librairie est capable de n’importer que les blocs du fichier qui l’intéresse.\nCeci nécessite l’utilisation de l’extension httpfs (un peu l’équivalent des library de R en duckdb). Elle s’installe et s’utilise de la manière suivante\n\ndbExecute(\n  con,\n  glue(\n    \"INSTALL httpfs;\",\n    \"LOAD httpfs;\"\n  )\n)\n\nDemandons à DuckDB d’exécuter la requête “beaucoup de colonnes, pas beaucoup de lignes” et regardons le plan d’exécution et les informations données par DuckDB:\n\nVoir le plan : “beaucoup de colonnes, pas beaucoup de lignes”\n\nglue(  \n    'EXPLAIN ANALYZE ',\n    'SELECT * FROM read_parquet(\"{url_bpe}\") LIMIT 5'\n  )\n\nEXPLAIN ANALYZE SELECT * FROM read_parquet(\"https://minio.lab.sspcloud.fr/lgaliana/diffusion/BPE23.parquet\") LIMIT 5\n\n\n\nplan &lt;- dbGetQuery(\n  con,\n  glue(  \n    'EXPLAIN ANALYZE ',\n    'SELECT * FROM read_parquet(\"{url_bpe}\") LIMIT 5'\n  )\n)\n\n\nprint(plan)\n\nanalyzed_plan\n┌─────────────────────────────────────┐\n│┌───────────────────────────────────┐│\n││    Query Profiling Information    ││\n│└───────────────────────────────────┘│\n└─────────────────────────────────────┘\nEXPLAIN ANALYZE SELECT * FROM read_parquet(\"https://minio.lab.sspcloud.fr/lgaliana/diffusion/BPE23.parquet\") LIMIT 5\n┌─────────────────────────────────────┐\n│┌───────────────────────────────────┐│\n││            HTTP Stats:            ││\n││                                   ││\n││           in: 164.5 MiB           ││\n││            out: 0 bytes           ││\n││              #HEAD: 1             ││\n││              #GET: 5              ││\n││              #PUT: 0              ││\n││              #POST: 0             ││\n│└───────────────────────────────────┘│\n└─────────────────────────────────────┘\n┌─────────────────────────────────────┐\n│┌───────────────────────────────────┐│\n││         Total Time: 16.70s        ││\n│└───────────────────────────────────┘│\n└─────────────────────────────────────┘\n┌───────────────────────────┐\n│      RESULT_COLLECTOR     │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             0             │\n│          (0.00s)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│      EXPLAIN_ANALYZE      │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             0             │\n│          (0.00s)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│           LIMIT           │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             5             │\n│          (0.00s)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│       READ_PARQUET        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             AN            │\n│           NOMRS           │\n│           CNOMRS          │\n│          NUMVOIE          │\n│           INDREP          │\n│          TYPVOIE          │\n│          LIBVOIE          │\n│            CADR           │\n│           CODPOS          │\n│           DEPCOM          │\n│            DEP            │\n│            REG            │\n│            DOM            │\n│            SDOM           │\n│           TYPEQU          │\n│           SIRET           │\n│      STATUT_DIFFUSION     │\n│          CANTINE          │\n│          INTERNAT         │\n│            RPI            │\n│             EP            │\n│           CL_PGE          │\n│            SECT           │\n│    ACCES_AIRE_PRATIQUE    │\n│        ACCES_LIBRE        │\n│      ACCES_SANITAIRE      │\n│      ACCES_VESTIAIRE      │\n│     CAPACITE_D_ACCUEIL    │\n│        PRES_DOUCHE        │\n└───────────────────────────┘                             \n\n\n\nVoir le plan : “peu de colonnes, beaucoup de lignes”\n\nplan &lt;- dbGetQuery(\n  con,\n  glue(  \n    'EXPLAIN ANALYZE ',\n    'SELECT TYPEQU, LONGITUDE, LATITUDE FROM read_parquet(\"{url_bpe}\") LIMIT 10000'\n  )\n)\n\n\nprint(plan)\n\nanalyzed_plan\n┌─────────────────────────────────────┐\n│┌───────────────────────────────────┐│\n││    Query Profiling Information    ││\n│└───────────────────────────────────┘│\n└─────────────────────────────────────┘\nEXPLAIN ANALYZE SELECT TYPEQU, LONGITUDE, LATITUDE FROM read_parquet(\"https://minio.lab.sspcloud.fr/lgaliana/diffusion/BPE23.parquet\") LIMIT 10000\n┌─────────────────────────────────────┐\n│┌───────────────────────────────────┐│\n││            HTTP Stats:            ││\n││                                   ││\n││            in: 38.5 MiB           ││\n││            out: 0 bytes           ││\n││              #HEAD: 1             ││\n││              #GET: 8              ││\n││              #PUT: 0              ││\n││              #POST: 0             ││\n│└───────────────────────────────────┘│\n└─────────────────────────────────────┘\n┌─────────────────────────────────────┐\n│┌───────────────────────────────────┐│\n││         Total Time: 8.28s         ││\n│└───────────────────────────────────┘│\n└─────────────────────────────────────┘\n┌───────────────────────────┐\n│      RESULT_COLLECTOR     │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             0             │\n│          (0.00s)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│      EXPLAIN_ANALYZE      │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             0             │\n│          (0.00s)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│           LIMIT           │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│           10000           │\n│          (0.00s)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│       READ_PARQUET        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│           TYPEQU          │\n│         LONGITUDE         │\n│          LATITUDE         │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│        EC: 2773420        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│           30720           │\n│          (11.14s)         │\n└───────────────────────────┘                             \n\n\nLa comparaison de ces plans d’exécution montre l’intérêt de faire un filtre sur les colonnes : les besoins computationnels sont drastiquement diminués. Le filtre sur les lignes n’arrive que dans un second temps, une fois les colonnes sélectionnées.\nPourquoi seulement un rapport de 1 à 4 entre le poids des deux fichiers ? C’est parce que nos requêtes comportent toute deux la variable IPONDI (les poids à utiliser pour extrapoler l’échantillon à la population) qui est à haute précision là où beaucoup d’autres colonnes comportent un nombre réduit de modalités et sont donc peu volumineuses.\n\n\n\nDuckDB propose également des fonctionnalités pour extraire des colonnes à travers des expressions régulières. Cette approche est également possible avec le tidyverse\n\n\nDuckDB via tidyverse\nDuckDB exclusivement\n\n\n\n\ntable_individu %&gt;%\n  select(poids = IPONDI, contains(\"AGE\")) %&gt;%\n  head(10)\n\n\n  \n\n\n\n\n\n\ndbGetQuery(\n  con,\n  glue(\n    \"FROM read_parquet(\\\"{filename_table_individu}\\\") \",\n    \"SELECT IPONDI AS poids, COLUMNS('.*AGE.*') \",\n    \"LIMIT 10\"\n  )\n)"
  },
  {
    "objectID": "tuto/r.html#requêtes-sur-les-lignes-where",
    "href": "tuto/r.html#requêtes-sur-les-lignes-where",
    "title": "Utilisation du format Parquet avec  illustré à partir de quelques exemples",
    "section": "Requêtes sur les lignes (WHERE)",
    "text": "Requêtes sur les lignes (WHERE)\n\n\nDuckDB via tidyverse\nDuckDB exclusivement\n\n\n\n\ntable_individu %&gt;%\n  filter(DEPT %in% c(\"11\", \"31\", \"34\")) %&gt;%\n  head(10)\n\n\n  \n\n\n\n\n\n\ndbGetQuery(\n  con,\n  glue(\n    \"FROM read_parquet(\\\"{filename_table_individu}\\\") \",\n    \"SELECT IPONDI, AGED, DEPT \",\n    \"WHERE DEPT IN ('11', '31', '34') \",\n    \"LIMIT 10\")\n)\n\n\n  \n\n\n\n\n\n\nLes filtres sur les observations peuvent être faits à partir de critères sur plusieurs colonnes. Par exemple, pour ne conserver que les observations de la ville de Nice où la date d’emménagement est postérieure à 2020, la requête suivante peut être utilisée :\n\n\nDuckDB via tidyverse\nDuckDB exclusivement\n\n\n\n\n\n\nListing 1: Ne conserver que les Niçois qui ont emménagé depuis 2021\n\ntable_logement %&gt;% filter(COMMUNE == \"06088\", AEMM &gt; 2020) %&gt;% collect() %&gt;% nrow(.)\n\n\n\n\n[1] 2692\n\n\n\n\n\ndbGetQuery(\n  con,\n  glue(\n    \"FROM read_parquet(\\\"{filename_table_logement}\\\") \",\n    \"SELECT * \",\n    \"WHERE COMMUNE = '06088' and AEMM &gt; 2020 \",\n    \"LIMIT 10\"\n  )\n)"
  },
  {
    "objectID": "tuto/r.html#exemples-sans-groupes",
    "href": "tuto/r.html#exemples-sans-groupes",
    "title": "Utilisation du format Parquet avec  illustré à partir de quelques exemples",
    "section": "Exemples sans groupes",
    "text": "Exemples sans groupes\nLa fonction DISTINCT appliquée à la variable ARM permet d’extraire la liste des codes arrondissements présents dans la base de données.\n\n\nDuckDB via tidyverse\nDuckDB exclusivement\n\n\n\n\ntable_logement %&gt;%\n  filter(str_detect(ARM, \"ZZZZZ\", negate = TRUE)) %&gt;%\n  summarise(ARM = distinct(ARM)) %&gt;%\n  arrange(ARM)\n\n\n  \n\n\n\n\n\n\nquery &lt;- glue_sql(\n    \"FROM read_parquet({filename_table_logement}) \",\n    \"SELECT DISTINCT(ARM) \",\n    \"WHERE NOT CONTAINS(ARM, 'ZZZZZ') \",\n    \"ORDER BY ARM\",\n    .con=con\n)\npaste(dbGetQuery(con, query)$ARM, collapse = \", \")\n\n[1] \"13201, 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13209, 13210, 13211, 13212, 13213, 13214, 13215, 13216, 69381, 69382, 69383, 69384, 69385, 69386, 69387, 69388, 69389, 75101, 75102, 75103, 75104, 75105, 75106, 75107, 75108, 75109, 75110, 75111, 75112, 75113, 75114, 75115, 75116, 75117, 75118, 75119, 75120\"\n\n\n\n\n\nIl est possible d’extraire des statistiques beaucoup plus raffinées par le biais d’une requête SQL plus complexe. Par exemple pour calculer le nombre d’habitants de Toulouse qui ont changé de logement en un an:\n\n\nDuckDB via tidyverse\nDuckDB exclusivement\n\n\n\n\n\n\nListing 2: Nombre de Toulousains qui ont changé de logement en un an\n\ntable_logement %&gt;%\n  filter(COMMUNE == '31555', !IRANM %in% c('1', 'Z'), INPER != \"Y\") %&gt;%\n  mutate(INPER = as.integer(INPER)) %&gt;%\n  summarise(habitants_toulouse_demenagement = as.integer(sum(IPONDL * INPER)))\n\n\n\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n\n  \n\n\n\n\n\n\nquery &lt;- glue(\n  \"FROM read_parquet(\\\"{filename_table_logement}\\\") \",\n  \"SELECT CAST(SUM(IPONDL*CAST(INPER AS INT)) AS INT) \",\n  \"AS habitants_toulouse_demenagement \",\n  \"WHERE COMMUNE == '31555' AND IRANM NOT IN ('1', 'Z') AND INPER != 'Y'\"\n)\ndbGetQuery(con, query)"
  },
  {
    "objectID": "tuto/r.html#statistiques-par-groupe",
    "href": "tuto/r.html#statistiques-par-groupe",
    "title": "Utilisation du format Parquet avec  illustré à partir de quelques exemples",
    "section": "Statistiques par groupe",
    "text": "Statistiques par groupe\nSQL et dplyr permettent d’aller loin dans la finesse des statistiques descriptives mises en oeuvre. Cela sera illustré à l’aide de plusieurs exemples réflétant des statistiques pouvant être construites grâce à ces données détaillées.\nExemple 1: pyramide des âges dans l’Aude, l’Hérault et le Gard\nLe premier exemple est un comptage sur trois départements. Il illustre la démarche suivante:\n\nOn se restreint aux observations d’intérêt (ici 3 départements)\nOn applique la fonction summarise pour calculer une statistique par groupe, en l’occurrence la somme des pondérations\nOn retravaille les données\n\nEnsuite, une fois que nos données sont récupérées dans R, on peut faire la figure avec ggplot\n\n\n\nListing 3: Pyramide des âges dans l’Aude, l’Hérault et le Gard\n\npyramide_ages &lt;- table_individu %&gt;%\n  filter(DEPT %in% c('11', '31', '34')) %&gt;%\n  group_by(AGED, departement = DEPT) %&gt;%\n  summarise(individus = sum(IPONDI), .groups = \"drop\") %&gt;%\n  arrange(departement, AGED) %&gt;%\n  collect()\n\n\nggplot(pyramide_ages, aes(x = AGED, y = individus)) +\n  geom_bar(aes(fill = departement), stat = \"identity\") +\n  geom_vline(xintercept = 18, color = \"grey\", linetype = \"dashed\") +\n  facet_wrap(~departement, scales = \"free_y\", nrow = 3) +\n  theme_minimal() +\n  labs(y = \"Individus recensés\", x = \"Âge\")\n\n\n\n\n\n\n\n\n\n\nExemple 2: répartition des plus de 60 ans par département\nL’objectif de ce deuxième exemple est d’illustrer la construction d’une statistique un peu plus complexe et la manière de projeter celle-ci sur une carte.\nPour avoir la répartition des plus de 60 ans par département, quelques lignes de dplyr suffisent:\n\n\n\nListing 4: Calculer la part des plus de 60 ans dans la population de chaque département\n\npart_population_60_plus &lt;- table_individu %&gt;%\n  group_by(DEPT) %&gt;%\n  summarise(\n    total_population = sum(IPONDI), # Population totale\n    population_60_plus = sum(IPONDI[AGED &gt; 60]) # Population de plus de 60 ans\n  ) %&gt;%\n  mutate(pourcentage_60_plus = population_60_plus / total_population * 100) %&gt;%\n  collect()\n\npart_population_60_plus\n\n\n\n\n\n  \n\n\n\nIl ne reste plus qu’à projeter ceci sur une carte. Pour cela, un join à notre fond de carte suffit. Comme les données sont agrégées et déjà dans R, il n’y a rien de spécifique à duckdb ici.\n\nAssociation de part_population_60_plus au fond de carte des départements# Joindre les données au fond de carte des départements\ndepartements_60_plus_sf &lt;- departements %&gt;%\n  inner_join(\n    part_population_60_plus,\n    by = c(\"INSEE_DEP\" = \"DEPT\")\n  )\n\n\nFinalement, il ne reste plus qu’à produire la carte:\n\nggplot(departements_60_plus_sf) +\n    geom_sf(aes(fill = pourcentage_60_plus)) + \n    scale_fill_fermenter(n.breaks = 5, palette = \"PuBuGn\", direction = 1) + \n    theme_void() + \n    labs(\n        title = \"Part des personnes de plus de 60 ans par département\",\n        caption = \"Source: Insee, Fichiers détails du recensement de la population\",\n        fill = \"Part (%)\"\n    )\n\n\n\n\n\n\n\nSi on préfère représenter ceci sous forme de tableau, on peut utiliser le package gt.\n\nCode pour avoir le classement des départements pour lesquels la population de plus de 60 ans est la plus importantetop_population &lt;- part_population_60_plus %&gt;%\n  left_join(\n    departements %&gt;% select(INSEE_DEP, LIBELLE_DEPARTEMENT ) %&gt;% st_set_geometry(NULL),\n    by = c(\"DEPT\" = \"INSEE_DEP\")\n  ) %&gt;%\n  mutate(departement = paste0(LIBELLE_DEPARTEMENT, \" (\", DEPT , \")\")) %&gt;%\n  select(-DEPT, -LIBELLE_DEPARTEMENT) %&gt;%\n  arrange(desc(pourcentage_60_plus)) %&gt;%\n  select(DEPT = departement, everything()) %&gt;%\n  head(10)\n\ngt(\n  top_population\n) %&gt;%\n  gt_plt_bar_pct(\n    column = pourcentage_60_plus,\n    scaled = TRUE,\n    labels = TRUE\n  ) %&gt;%\n    fmt_number(\n    columns = c(\"total_population\", \"population_60_plus\"),\n    decimals = 0,\n    sep_mark = \" \"\n  ) %&gt;%\n  fmt_number(\n      columns = c(\"pourcentage_60_plus\"),\n      decimals = 1\n    ) %&gt;%\n  cols_label(\n    DEPT = md(\"**Département**\"),\n    total_population = md(\"**Population**\"),\n    population_60_plus = md(\"**Population de plus de 60 ans**\"),\n    pourcentage_60_plus = md(\"*Part (%)*\")\n  )\n\n\n\n\n\nDépartement\nPopulation\nPopulation de plus de 60 ans\nPart (%)\n\n\n\nCreuse (23)\n116 178\n45 561\n\n\n39.2%\n\n\n\n\nLot (46)\n174 397\n67 122\n\n\n38.5%\n\n\n\n\nNièvre (58)\n202 728\n76 108\n\n\n37.5%\n\n\n\n\nDordogne (24)\n413 180\n153 405\n\n\n37.1%\n\n\n\n\nCantal (15)\n144 321\n51 594\n\n\n35.7%\n\n\n\n\nGers (32)\n191 737\n68 261\n\n\n35.6%\n\n\n\n\nIndre (36)\n218 469\n77 537\n\n\n35.5%\n\n\n\n\nCharente-Maritime (17)\n655 648\n232 006\n\n\n35.4%\n\n\n\n\nAllier (03)\n335 380\n118 481\n\n\n35.3%\n\n\n\n\nCorrèze (19)\n239 286\n84 511\n\n\n35.3%\n\n\n\n\n\n\n\n\nExemple 3: part des résidences secondaires et des logements vacants\nIl est tout à fait possible de faire des étapes antérieures de préparation de données, notamment de création de variables avec mutate.\nL’exemple suivant illustre la préparation de données avant la construction de statistiques descriptives de la manière suivante:\n\nCréation d’une variable de département à partir du code commune\nDécompte des logements par département\n\n\n\n\nListing 5: Part des logements vacants et résidences secondaires dans le parc de logement\n\n#| output: false\nparc_locatif &lt;- table_logement %&gt;%\n  mutate(DEPT = substring(COMMUNE, 1, 3)) %&gt;%\n  mutate(\n    DEPT = if_else(\n      starts_with(DEPT, \"97\"),\n      DEPT,\n      substring(DEPT, 1, 2)\n    )\n  ) %&gt;%\n  group_by(DEPT, CATL) %&gt;%\n  summarise(n = sum(IPONDL)) %&gt;%\n  ungroup() %&gt;%\n  collect()\n\n\n\n\n`summarise()` has grouped output by \"DEPT\". You can override using the\n`.groups` argument.\n\n\n\n# Jointure avec le fond de carte des départements\nparc_locatif_sf &lt;- departements %&gt;%\n  inner_join(\n    parc_locatif,\n    by = c(\"INSEE_DEP\" = \"DEPT\"),\n    relationship = \"many-to-many\"\n  ) %&gt;%\n  group_by(INSEE_DEP) %&gt;%\n  mutate(p = n/sum(n)) %&gt;%\n  ungroup\n\n\n1\n\nOn a des clés dupliquées dans le fond cartiflette (le zoom pour l’Ile de France) et dans le dataframe (4 valeurs par dep)\n\n\n\n\nCode pour produire la carte# Carte: Part des résidences secondaires\ncarte1 &lt;- ggplot(parc_locatif_sf %&gt;% filter(CATL == \"3\")) +\n  geom_sf(aes(fill = p), color = \"white\") +\n  scale_fill_fermenter(\n    n.breaks = 5, \n    palette = \"RdPu\",\n    direction = 1,\n    labels = scales::label_percent(\n      scale_cut = scales::cut_short_scale()\n    )\n  ) +\n  theme_void() +\n  labs(\n    fill = \"Part dans le\\nparc de logement (%)\",\n    title = \"Cartographie des résidences secondaires\",\n    caption = \"Source: Insee, Fichiers détails du recensement de la population\"\n  )\n\n# Carte: Part des logements vacants\ncarte2 &lt;- ggplot(parc_locatif_sf %&gt;% filter(CATL == \"4\")) +\n  geom_sf(aes(fill = p), color = \"white\") +\n  scale_fill_fermenter(\n    n.breaks = 5, \n    palette = \"RdPu\",\n    direction = 1,\n    labels = scales::label_percent(\n      scale_cut = scales::cut_short_scale()\n    )\n  ) +\n  theme_void() +\n  labs(\n    fill = \"Part dans le\\nparc de logement (%)\",\n    title = \"Cartographie des logements vacants\",\n    caption = \"Source: Insee, Fichiers détails du recensement de la population\"\n  )\n\n\ncarte1\ncarte2\n\n\n\n\n\nRésidences secondaires\n\n\n\n\n\nLogements vacants"
  },
  {
    "objectID": "tuto/r.html#footnotes",
    "href": "tuto/r.html#footnotes",
    "title": "Utilisation du format Parquet avec  illustré à partir de quelques exemples",
    "section": "Footnotes",
    "text": "Footnotes\n\nSi vous avez clôné le dépôt disponible sur Github R, un environnement virtuel renv vous permet de recréer la même configuration logicielle que celle utilisée pour générer cette page. Pour cela, il suffit de faire renv::restore().↩︎\nPour en savoir plus sur ce projet, se rendre sur la documentation du projet.↩︎\nCe filtre est construit après lecture du dictionnaire des variables de la BPE.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Atelier tuto@mate sur le format Parquet",
    "section": "",
    "text": "Un ensemble de ressources pour présenter les enjeux de l’utilisation du format Parquet illustré à partir des données du recensement."
  },
  {
    "objectID": "index.html#tutoriels",
    "href": "index.html#tutoriels",
    "title": "Atelier tuto@mate sur le format Parquet",
    "section": "Tutoriels",
    "text": "Tutoriels\n\n\n\n\n\n\n\n\n\n\nUtilisation du format Parquet avec  illustré à partir de quelques exemples\n\n\nTutoriel R\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation du format Parquet avec Python illustré à partir de quelques exemples\n\n\nTutoriel Python\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilisation du format Parquet avec Observable illustré à partir de quelques exemples\n\n\nTutoriel Observable\n\n\n\nLino Galiana\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#replay-de-la-présentation",
    "href": "index.html#replay-de-la-présentation",
    "title": "Atelier tuto@mate sur le format Parquet",
    "section": "Replay de la présentation",
    "text": "Replay de la présentation"
  },
  {
    "objectID": "tuto/observablehq.html",
    "href": "tuto/observablehq.html",
    "title": "Utilisation du format Parquet avec Observable illustré à partir de quelques exemples",
    "section": "",
    "text": "Page observablehq"
  },
  {
    "objectID": "tuto/python.html",
    "href": "tuto/python.html",
    "title": "Utilisation du format Parquet avec Python illustré à partir de quelques exemples",
    "section": "",
    "text": "Ce tutoriel n’est pas encore fini, il sera progressivement enrichi\nCe tutoriel vise à offrir une approche complémentaire au guide d’utilisation des données du recensement au format Parquet publié sur https://ssphub.netlify.app pour accompagner la diffusion de celles-ci par l’Insee.\nIl s’agit d’un tutoriel préparé pour l’atelier tuto@mate à l’EHESS le 5 novembre 2024. Ce tutoriel est exclusivement en R. Les slides de la présentation sont disponibles ci-dessous:\nDérouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein écran.\nIl propose des exemples variés pour illustrer la simplicité d’usage du format Parquet. Parmi ceux-ci, à partir du recensement de la population:"
  },
  {
    "objectID": "tuto/python.html#requêtes-sur-les-colonnes-select",
    "href": "tuto/python.html#requêtes-sur-les-colonnes-select",
    "title": "Utilisation du format Parquet avec Python illustré à partir de quelques exemples",
    "section": "Requêtes sur les colonnes (SELECT)",
    "text": "Requêtes sur les colonnes (SELECT)\nL’une des forces du format Parquet est de simplifier l’import de fichiers volumineux qui ne comportent que quelques colonnes nous intéressant. Par exemple, la table des individus comporte 88 colonnes, il est peu probable qu’une seule analyse s’intéresse à toutes celles-ci (ou elle risque d’être fort indigeste).\n\nComme cela est illustré dans ?@tip-optimisation-duckdb, la différence de volumétrie entre un fichier non filtré et un fichier filtré est importante.\n\nquery = (\n  f\"FROM read_parquet(\\\"{filename_table_individu}\\\") \"\n  \"SELECT IPONDI AS poids, AGED, VOIT \"\n  \"LIMIT 10\"\n)\nduckdb.sql(query).to_df()\n\n\n\n\n\n\n\n\npoids\nAGED\nVOIT\n\n\n\n\n0\n0.865066\n74\n0\n\n\n1\n4.987931\n39\n1\n\n\n2\n4.987931\n9\n1\n\n\n3\n5.013547\n55\n2\n\n\n4\n3.478368\n1\n0\n\n\n5\n3.478368\n43\n0\n\n\n6\n3.478368\n38\n0\n\n\n7\n1.003585\n46\n1\n\n\n8\n1.003585\n16\n1\n\n\n9\n0.984087\n59\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\nComprendre l’optimisation permise par Parquet et DuckDB\n\n\n\n\n\nPour réduire la volumétrie des données importées, il est possible de mettre en oeuvre deux stratégies:\n\nN’importer qu’un nombre limité de colonnes\nN’importer qu’un nombre limité de lignes\n\nComme cela a été évoqué dans les slides, le format Parquet est particulièrement optimisé pour le premier besoin. C’est donc généralement la première optimisation mise en oeuvre. Pour s’en convaincre on peut regarder la taille des données importées dans deux cas:\n\nOn utilise beaucoup de lignes mais peu de colonnes\nOn utilise beaucoup de colonnes mais peu de lignes\n\nPour cela, nous utilisons la fonction SQL EXPLAIN ANALYZE disponible dans duckdb. Elle décompose le plan d’exécution de duckdb, ce qui nous permettra de comprendre la stratégie d’optimisation. Elle permet aussi de connaître le volume de données importées lorsqu’on récupère un fichier d’internet. En effet, duckdb est malin: plutôt que de télécharger un fichier entier pour n’en lire qu’une partie, la librairie est capable de n’importer que les blocs du fichier qui l’intéresse.\nCeci nécessite l’utilisation de l’extension httpfs (un peu l’équivalent des library de R en duckdb). Elle s’installe et s’utilise de la manière suivante\n\nduckdb.sql(\"INSTALL httpfs; LOAD httpfs;\")\n\nDemandons à DuckDB d’exécuter la requête “beaucoup de colonnes, pas beaucoup de lignes” et regardons le plan d’exécution et les informations données par DuckDB:\n\n\nVoir le plan : “beaucoup de colonnes, pas beaucoup de lignes”\n\n\nurl_bpe = files_to_download.get(\"table_bpe\").get(\"url\")\n\nplan = duckdb.query(\nf\"\"\"\nEXPLAIN ANALYZE\nFROM read_parquet(\"{url_bpe}\")\nSELECT *\nLIMIT 5\n\"\"\"\n) \n\n\nprint(\n  plan.to_df()['explain_value'].iloc[0]\n)\n\n┌─────────────────────────────────────┐\n│┌───────────────────────────────────┐│\n││    Query Profiling Information    ││\n│└───────────────────────────────────┘│\n└─────────────────────────────────────┘\n EXPLAIN ANALYZE FROM read_parquet(\"https://minio.lab.sspcloud.fr/lgaliana/diffusion/BPE23.parquet\") SELECT * LIMIT 5 \n┌─────────────────────────────────────┐\n│┌───────────────────────────────────┐│\n││         HTTPFS HTTP Stats         ││\n││                                   ││\n││            in: 60.2 MiB           ││\n││            out: 0 bytes           ││\n││              #HEAD: 1             ││\n││              #GET: 3              ││\n││              #PUT: 0              ││\n││              #POST: 0             ││\n│└───────────────────────────────────┘│\n└─────────────────────────────────────┘\n┌────────────────────────────────────────────────┐\n│┌──────────────────────────────────────────────┐│\n││               Total Time: 4.56s              ││\n│└──────────────────────────────────────────────┘│\n└────────────────────────────────────────────────┘\n┌───────────────────────────┐\n│           QUERY           │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│      EXPLAIN_ANALYZE      │\n│    ────────────────────   │\n│           0 Rows          │\n│          (0.00s)          │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│      STREAMING_LIMIT      │\n│    ────────────────────   │\n│           5 Rows          │\n│          (0.00s)          │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│         TABLE_SCAN        │\n│    ────────────────────   │\n│         Function:         │\n│        READ_PARQUET       │\n│                           │\n│        Projections:       │\n│             AN            │\n│           NOMRS           │\n│           CNOMRS          │\n│          NUMVOIE          │\n│           INDREP          │\n│          TYPVOIE          │\n│          LIBVOIE          │\n│            CADR           │\n│           CODPOS          │\n│           DEPCOM          │\n│            DEP            │\n│            REG            │\n│            DOM            │\n│            SDOM           │\n│           TYPEQU          │\n│           SIRET           │\n│      STATUT_DIFFUSION     │\n│          CANTINE          │\n│          INTERNAT         │\n│            RPI            │\n│             EP            │\n│           CL_PGE          │\n│            SECT           │\n│    ACCES_AIRE_PRATIQUE    │\n│        ACCES_LIBRE        │\n└───────────────────────────┘\n\n\n\n\n\n\nVoir le plan : “pas de colonnes, beaucoup de lignes”\n\n\nplan = duckdb.sql(\n  f\"\"\"\n  EXPLAIN ANALYZE\n  SELECT TYPEQU, LONGITUDE, LATITUDE FROM read_parquet(\"{url_bpe}\") LIMIT 10000\n  \"\"\"\n)\n\n\nprint(\n  plan.to_df()['explain_value'].iloc[0]\n)\n\n┌─────────────────────────────────────┐\n│┌───────────────────────────────────┐│\n││    Query Profiling Information    ││\n│└───────────────────────────────────┘│\n└─────────────────────────────────────┘\n   EXPLAIN ANALYZE   SELECT TYPEQU, LONGITUDE, LATITUDE FROM read_parquet(\"https://minio.lab.sspcloud.fr/lgaliana/diffusion/BPE23.parquet\") LIMIT 10000   \n┌─────────────────────────────────────┐\n│┌───────────────────────────────────┐│\n││         HTTPFS HTTP Stats         ││\n││                                   ││\n││            in: 13.8 MiB           ││\n││            out: 0 bytes           ││\n││              #HEAD: 1             ││\n││              #GET: 4              ││\n││              #PUT: 0              ││\n││              #POST: 0             ││\n│└───────────────────────────────────┘│\n└─────────────────────────────────────┘\n┌────────────────────────────────────────────────┐\n│┌──────────────────────────────────────────────┐│\n││               Total Time: 1.65s              ││\n│└──────────────────────────────────────────────┘│\n└────────────────────────────────────────────────┘\n┌───────────────────────────┐\n│           QUERY           │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│      EXPLAIN_ANALYZE      │\n│    ────────────────────   │\n│           0 Rows          │\n│          (0.00s)          │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│      STREAMING_LIMIT      │\n│    ────────────────────   │\n│         10000 Rows        │\n│          (0.00s)          │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│         TABLE_SCAN        │\n│    ────────────────────   │\n│         Function:         │\n│        READ_PARQUET       │\n│                           │\n│        Projections:       │\n│           TYPEQU          │\n│         LONGITUDE         │\n│          LATITUDE         │\n│                           │\n│         12288 Rows        │\n│          (1.65s)          │\n└───────────────────────────┘\n\n\n\n\nLa comparaison de ces plans d’exécution montre l’intérêt de faire un filtre sur les colonnes : les besoins computationnels sont drastiquement diminués. Le filtre sur les lignes n’arrive que dans un second temps, une fois les colonnes sélectionnées.\nPourquoi seulement un rapport de 1 à 4 entre le poids des deux fichiers ? C’est parce que nos requêtes comportent toute deux la variable IPONDI (les poids à utiliser pour extrapoler l’échantillon à la population) qui est à haute précision là où beaucoup d’autres colonnes comportent un nombre réduit de modalités et sont donc peu volumineuses.\n\n\n\nDuckDB propose également des fonctionnalités pour extraire des colonnes à travers des expressions régulières. Cette approche est également possible avec le tidyverse\n\nduckdb.sql(\n  f\"\"\"\n    FROM read_parquet(\\\"{filename_table_individu}\\\")\n    SELECT IPONDI AS poids, COLUMNS('.*AGE.*')\n    LIMIT 10\n  \"\"\"\n).to_df()\n\n\n\n\n\n\n\n\npoids\nAGED\nAGER20\nAGEREV\nAGEREVQ\n\n\n\n\n0\n0.865066\n74\n79\n73\n70\n\n\n1\n4.987931\n39\n39\n38\n35\n\n\n2\n4.987931\n9\n10\n8\n5\n\n\n3\n5.013547\n55\n54\n54\n50\n\n\n4\n3.478368\n1\n2\n0\n0\n\n\n5\n3.478368\n43\n54\n42\n40\n\n\n6\n3.478368\n38\n39\n37\n35\n\n\n7\n1.003585\n46\n54\n45\n45\n\n\n8\n1.003585\n16\n17\n15\n15\n\n\n9\n0.984087\n59\n64\n58\n55"
  },
  {
    "objectID": "tuto/python.html#requêtes-sur-les-lignes-where",
    "href": "tuto/python.html#requêtes-sur-les-lignes-where",
    "title": "Utilisation du format Parquet avec Python illustré à partir de quelques exemples",
    "section": "Requêtes sur les lignes (WHERE)",
    "text": "Requêtes sur les lignes (WHERE)\n\nduckdb.sql(\n    f\"\"\"\n    FROM read_parquet(\"{filename_table_individu}\")\n    SELECT IPONDI, AGED, DEPT\n    WHERE DEPT IN ('11', '31', '34')\n    LIMIT 10\n    \"\"\"\n).to_df()\n\n\n\n\n\n\n\n\nIPONDI\nAGED\nDEPT\n\n\n\n\n0\n5.027778\n59\n11\n\n\n1\n5.027778\n83\n11\n\n\n2\n4.996593\n50\n11\n\n\n3\n4.996593\n26\n11\n\n\n4\n5.148676\n9\n11\n\n\n5\n5.148676\n38\n11\n\n\n6\n5.148676\n49\n11\n\n\n7\n5.000000\n88\n11\n\n\n8\n5.000000\n88\n11\n\n\n9\n5.297153\n18\n11\n\n\n\n\n\n\n\nLes filtres sur les observations peuvent être faits à partir de critères sur plusieurs colonnes. Par exemple, pour ne conserver que les observations de la ville de Nice où la date d’emménagement est postérieure à 2020, la requête suivante peut être utilisée :\n\nduckdb.sql(\n    f\"\"\"\n    FROM read_parquet(\"{filename_table_logement}\")\n    SELECT *\n    WHERE COMMUNE = '06088' AND AEMM &gt; 2020\n    LIMIT 10\n    \"\"\"\n).to_df()\n\n\n\n\n\n\n\n\nCOMMUNE\nARM\nIRIS\nACHL\nAEMM\nAEMMR\nAGEMEN8\nANEM\nANEMR\nASCEN\n...\nSTOCD\nSURF\nTACTM\nTPM\nTRANSM\nTRIRIS\nTYPC\nTYPL\nVOIT\nWC\n\n\n\n\n0\n06088\nZZZZZ\n060880101\nA11\n2022\n9\n40\n0\n0\n2\n...\n21\n4\n11\n1\n5\n060421\n3\n2\n1\nZ\n\n\n1\n06088\nZZZZZ\n060880101\nA11\n2021\n9\n40\n1\n0\n2\n...\n10\n2\n11\n1\n6\n060421\n3\n2\n0\nZ\n\n\n2\n06088\nZZZZZ\n060880101\nA12\n2021\n9\n25\n1\n0\n2\n...\n21\n2\n12\nZ\nZ\n060421\n3\n2\n0\nZ\n\n\n3\n06088\nZZZZZ\n060880101\nA11\n2021\n9\n40\n1\n0\n2\n...\n21\n3\n11\n1\n5\n060421\n3\n2\n1\nZ\n\n\n4\n06088\nZZZZZ\n060880101\nA11\n2021\n9\n40\n1\n0\n2\n...\n21\n3\n11\n1\n5\n060421\n3\n2\n1\nZ\n\n\n5\n06088\nZZZZZ\n060880101\nA11\n2021\n9\n40\n1\n0\n2\n...\n21\n2\n11\n1\n2\n060421\n3\n2\n0\nZ\n\n\n6\n06088\nZZZZZ\n060880101\nA11\n2021\n9\n25\n1\n0\n2\n...\n21\n2\n11\n1\n2\n060421\n2\n2\n0\nZ\n\n\n7\n06088\nZZZZZ\n060880101\nA11\n2021\n9\n40\n1\n0\n2\n...\n21\n2\n11\n1\n2\n060421\n3\n2\n0\nZ\n\n\n8\n06088\nZZZZZ\n060880101\nA11\n2021\n9\n25\n1\n0\n2\n...\n21\n5\n11\n1\n6\n060421\n3\n2\n1\nZ\n\n\n9\n06088\nZZZZZ\n060880101\nA11\n2021\n9\n55\n1\n0\n2\n...\n21\n3\n12\nZ\nZ\n060421\n2\n2\n0\nZ\n\n\n\n\n10 rows × 69 columns"
  },
  {
    "objectID": "tuto/python.html#exemples-sans-groupes",
    "href": "tuto/python.html#exemples-sans-groupes",
    "title": "Utilisation du format Parquet avec Python illustré à partir de quelques exemples",
    "section": "Exemples sans groupes",
    "text": "Exemples sans groupes\nLa fonction DISTINCT appliquée à la variable ARM permet d’extraire la liste des codes arrondissements présents dans la base de données.\n\nquery = f\"\"\"\nFROM read_parquet('{filename_table_logement}')\nSELECT DISTINCT ARM\nWHERE NOT ARM LIKE '%ZZZZZ%'\nORDER BY ARM\n\"\"\"\n\nresult = \", \".join(duckdb.sql(query).to_df()[\"ARM\"])\n\nIl est possible d’extraire des statistiques beaucoup plus raffinées par le biais d’une requête SQL plus complexe. Par exemple pour calculer le nombre d’habitants de Toulouse qui ont changé de logement en un an:\n\nquery = f\"\"\"\nFROM read_parquet(\"{filename_table_logement}\")\nSELECT CAST(SUM(IPONDL * CAST(INPER AS INT)) AS INT) \nAS habitants_toulouse_demenagement\nWHERE COMMUNE = '31555' \nAND IRANM NOT IN ('1', 'Z') \nAND INPER != 'Y'\n\"\"\"\n\nduckdb.sql(query).to_df()\n\n\n\n\n\n\n\n\nhabitants_toulouse_demenagement\n\n\n\n\n0\n86364"
  },
  {
    "objectID": "tuto/python.html#statistiques-par-groupe",
    "href": "tuto/python.html#statistiques-par-groupe",
    "title": "Utilisation du format Parquet avec Python illustré à partir de quelques exemples",
    "section": "Statistiques par groupe",
    "text": "Statistiques par groupe\nSQL et dplyr permettent d’aller loin dans la finesse des statistiques descriptives mises en oeuvre. Cela sera illustré à l’aide de plusieurs exemples réflétant des statistiques pouvant être construites grâce à ces données détaillées.\n\nExemple 1: pyramide des âges dans l’Aude, l’Hérault et le Gard\nLe premier exemple est un comptage sur trois départements. Il illustre la démarche suivante:\n\nOn se restreint aux observations d’intérêt (ici 3 départements)\nOn applique la fonction summarise pour calculer une statistique par groupe, en l’occurrence la somme des pondérations\nOn retravaille les données\n\nEnsuite, une fois que nos données sont récupérées dans R, on peut faire la figure avec ggplot\n\n\n\n\nListing 1: Pyramide des âges dans l’Aude, l’Hérault et le Gard\n\n\npyramide_ages = duckdb.sql(\n        f\"\"\"\n        FROM read_parquet(\"{filename_table_individu}\")\n        SELECT AGED, DEPT AS departement, SUM(IPONDI) AS individus\n        WHERE DEPT IN ('11', '31', '34')\n        GROUP BY AGED, DEPT\n        ORDER BY DEPT, AGED\n        \"\"\"\n    ).to_df()\n\n# Create the plot\nplot = (\n    ggplot(pyramide_ages, aes(x='AGED', y='individus'))\n    + geom_bar(aes(fill='departement'), stat=\"identity\")\n    + geom_vline(xintercept=18, color=\"grey\", linetype=\"dashed\")\n    + facet_wrap('~departement', scales=\"free_y\", nrow=3)\n    + theme_minimal()\n    + labs(y=\"Individus recensés\", x=\"Âge\")\n)\n\nplot\n\n\n\n\n\n\n\n\n\n\n\n\n\nExemple 2: répartition des plus de 60 ans par département\nL’objectif de ce deuxième exemple est d’illustrer la construction d’une statistique un peu plus complexe et la manière de projeter celle-ci sur une carte.\nPour avoir la répartition des plus de 60 ans par département, quelques lignes de dplyr suffisent:\n\nimport pandas as pd\nimport duckdb\n\n# Query to calculate total population and population over 60\npart_population_60_plus = duckdb.sql(\n    f\"\"\"\n    FROM read_parquet(\"{filename_table_individu}\")\n    SELECT \n        DEPT,\n        SUM(IPONDI) AS total_population,\n        SUM(CASE WHEN AGED &gt; 60 THEN IPONDI ELSE 0 END) AS population_60_plus\n    GROUP BY DEPT\n    \"\"\"\n).to_df()\n\n# Calculate percentage of population over 60\npart_population_60_plus['pourcentage_60_plus'] = (\n    part_population_60_plus['population_60_plus'] / part_population_60_plus['total_population'] * 100\n)\n\n# Display the result\npart_population_60_plus\n\n\n\n\n\n\n\n\nDEPT\ntotal_population\npopulation_60_plus\npourcentage_60_plus\n\n\n\n\n0\n15\n1.443207e+05\n51594.268330\n35.749725\n\n\n1\n16\n3.515488e+05\n115884.210207\n32.963901\n\n\n2\n22\n6.034105e+05\n201335.103066\n33.366193\n\n\n3\n24\n4.131804e+05\n153404.822502\n37.127805\n\n\n4\n45\n6.827497e+05\n178146.809475\n26.092549\n\n\n...\n...\n...\n...\n...\n\n\n95\n29\n9.170858e+05\n277937.304226\n30.306575\n\n\n96\n2A\n1.607982e+05\n49182.373626\n30.586396\n\n\n97\n60\n8.296354e+05\n191667.259892\n23.102590\n\n\n98\n61\n2.782949e+05\n93452.962115\n33.580552\n\n\n99\n77\n1.428738e+06\n287073.307178\n20.092786\n\n\n\n\n100 rows × 4 columns\n\n\n\nIl ne reste plus qu’à projeter ceci sur une carte. Pour cela, un join à notre fond de carte suffit. Comme les données sont agrégées et déjà dans R, il n’y a rien de spécifique à duckdb ici.\n\n\nAssociation de part_population_60_plus au fond de carte des départements\n# Joindre les données au fond de carte des départements\n\ndepartements_60_plus_gpd = (\n  departements\n    .merge(\n      part_population_60_plus,\n      left_on = \"INSEE_DEP\",\n      right_on = \"DEPT\"\n    )\n)\n\n\nFinalement, il ne reste plus qu’à produire la carte:\n\ndepartements_60_plus_gpd['pourcentage_60_plus_d'] = pd.qcut(\n  departements_60_plus_gpd['pourcentage_60_plus'],\n  q=4\n)\n\n\n(\n  ggplot(departements_60_plus_gpd) +\n    geom_map(aes(fill = \"pourcentage_60_plus_d\")) + \n    scale_fill_brewer(palette = \"PuBuGn\", direction = 1) + \n    theme_void() + \n    labs(\n        title = \"Part des personnes de plus de 60 ans par département\",\n        caption = \"Source: Insee, Fichiers détails du recensement de la population\",\n        fill = \"Part (%)\"\n    )\n)\n\n\n\n\n\n\n\n\nSi on préfère représenter ceci sous forme de tableau, on peut utiliser le package great tables. Cela nécessite quelques manipulations de données en amont.\n\npart_population_60_plus_dep = part_population_60_plus.merge(\n  departements.loc[:, [\"INSEE_DEP\", \"LIBELLE_DEPARTEMENT\"]],\n  left_on = \"DEPT\",\n  right_on = \"INSEE_DEP\"\n)\npart_population_60_plus_dep[\"departement\"] = part_population_60_plus_dep[\"LIBELLE_DEPARTEMENT\"] + \" (\" + part_population_60_plus_dep[\"DEPT\"]  + \")\"\n\npart_population_60_plus_dep = part_population_60_plus_dep.sort_values(\"pourcentage_60_plus\", ascending=False).head(10)\n\n\nfrom great_tables import *\n(\n  GT(\n    part_population_60_plus_dep.loc[\n      :, ['departement', 'total_population', 'population_60_plus', 'pourcentage_60_plus']\n    ]\n  )\n  .fmt_number(columns=[\n    \"total_population\", \"population_60_plus\"\n  ], compact=True)\n  .fmt_percent(\"pourcentage_60_plus\", scale_values = False)\n  .fmt_nanoplot(columns=\"pourcentage_60_plus\", plot_type=\"bar\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\ndepartement\ntotal_population\npopulation_60_plus\npourcentage_60_plus\n\n\n\n\nCreuse (23)\n116.18K\n45.56K\n\n\n\n\n39.2\n\n\n\n\nLot (46)\n174.40K\n67.12K\n\n\n\n\n38.5\n\n\n\n\nNièvre (58)\n202.73K\n76.11K\n\n\n\n\n37.5\n\n\n\n\nDordogne (24)\n413.18K\n153.40K\n\n\n\n\n37.1\n\n\n\n\nCantal (15)\n144.32K\n51.59K\n\n\n\n\n35.7\n\n\n\n\nGers (32)\n191.74K\n68.26K\n\n\n\n\n35.6\n\n\n\n\nIndre (36)\n218.47K\n77.54K\n\n\n\n\n35.5\n\n\n\n\nCharente-Maritime (17)\n655.65K\n232.01K\n\n\n\n\n35.4\n\n\n\n\nAllier (03)\n335.38K\n118.48K\n\n\n\n\n35.3\n\n\n\n\nCorrèze (19)\n239.29K\n84.51K\n\n\n\n\n35.3\n\n\n\n\n\n\n\n\n        \n\n\n\n\nExemple 3: part des résidences secondaires et des logements vacants\nIl est tout à fait possible de faire des étapes antérieures de préparation de données, notamment de création de variables avec mutate.\nL’exemple suivant illustre la préparation de données avant la construction de statistiques descriptives de la manière suivante:\n\nCréation d’une variable de département à partir du code commune\nDécompte des logements par département\n\nSuite du tutoriel à venir\n\nimport shutil\nshutil.rmtree(\"data\")"
  },
  {
    "objectID": "tuto/python.html#footnotes",
    "href": "tuto/python.html#footnotes",
    "title": "Utilisation du format Parquet avec Python illustré à partir de quelques exemples",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPour en savoir plus sur ce projet, se rendre sur la documentation du projet.↩︎"
  }
]